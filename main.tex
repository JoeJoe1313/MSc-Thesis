\documentclass{mldsmsc}
% \setlength{\parindent}{20pt}

\title{Probabilistic Sequential Matrix Factorisation for 12-lead ECG Data}
\author{Joana Levtcheva}
\CID{01252821}
\supervisor{Dr Deniz Akyildiz}
% \date{1 May 2023}
%For today's date, use:
\date{\today}
\logoimg{}


% THIS IS WHERE NEW COMMANDS CAN BE DEFINED
% commands below only used in the proof; otherwise can be deleted
\newcommand{\consta}{a}
\newcommand{\X}{X}
\newcommand{\EE}[1]{ \mathrm{E} [ #1 ] }
\newcommand{\inparenth}[1]{\left( #1 \right)}

\begin{document}

% Generates the Title Page
\maketitle


% Generates plagiarism declaration
\declarationname{Joana Levtcheva}
\declarationdate{\today}
\declaration 

\begin{abstract}
Probabilistic matrix factorisation for high-dimensional time-series data with nonlinear subspace: 12-lead ECG data exploration \newline

\noindent Example: In recent years, matrix factorisation (MF) algorithms have gained popularity in the field of machine learning and have found applications in various fields, such as recommendation systems, image processing and missing data imputation. The MF problem concerns factorising a high dimensional data matrix into two lower dimensional factors. The aim of MF methods is to find the factors such that we have a lower dimensional representation of the data.\newline

\noindent In this thesis, we propose a novel \newline

\noindent The main focus of the research project would be to explore PMF methods applied to high-dimensional time-series data with nonlinear subspace: both considering the time varying component, and treating the whole time series as a batch without considering the time component. More precisely, the project would focus on applying such methods to 12-lead ECG data which is 12-dimensional time-series data with a nonlinear subspace. Some of the tasks which could be explored include: modelling the data so that the future behaviour can be predicted, anomaly detection, imputing missing data, or performing peak detection. \newline

\noindent Using probabilistic methods (and specifically the PMF ones) on ECG data is not widely explored. Applying methods such as Probabilistic Matrix Factorisation (PMF) [1], Probabilistic Sequential Matrix Factorisation (PSMF) [2], Matrix-Variate Gaussian Matrix Factorization (MVGMF) [3], etc. to 12-lead ECG data would showcase novel real-world PMF applications in the medical field, and potentially improve the accuracy and efficiency of ECG data analysis. \newline

\end{abstract}

\begin{acknowledgements}

TODO 

I would like to...

\end{acknowledgements}

% add glossary?

% table of contents
\tableofcontents

% VERY IMPORTANT
% This command switches from Roman to Arabic numbering for main part of thesis
\mainmatter


\chapter{Introduction}

Matrix factorisation (or also matrix decomposition) in the context of linear algebra is simply a factorisation of a matrix into a product of multiple matrices. Many different decompositions exist, and they find various applications in mathematical problems such as solving systems of linear equations, matrix inversion, determinant computation, eigenvalues problems, solving systems of first order ODEs, etc. \newline

\noindent In this thesis, we are interested in matrix factorisation (MF) in the context of machine learning. Nowadays, MF techniques are highly effective and widely used in unsupervised machine learning. These methods aim to decompose the original matrix into multiple lower-dimensional matrices. By breaking the matrix in these simpler components MF aims to uncover latent structures that are not immediately apparent in the original matrix. Some applications are in image processing: for reducing dimensionality and noise in images, NLP for topic modelling, missing data imputation, recommendation systems, etc. \newline

\noindent Formally, we are interested in the general problem of factorising a data matrix $Y \in \mathbb{R}^{m \times n}$ as

\begin{equation}
Y \approx CX,
\end{equation} \newline
\noindent where $C \in \mathbb{R}^{m \times r}$ is the \textit{dictionary matrix}, $X \in \mathbb{R}^{r \times n}$ is the \textit{coefficients matrix} (with columns the coefficients), and $r$ is the \textit{approximation rank} (\cite{cite-key}). Visually we can present the problem as

\begin{equation}
\underbrace{
\begin{bmatrix}
  \times & \times & \times & \times & \times \\
  \times & \times & \times & \times & \times \\
  \times & \times & \times & \times & \times
\end{bmatrix}
}_{\text{Y $\in \mathbb{R}^{m \times n}$ }}
\approx
\underbrace{
\begin{bmatrix}
  \times & \times \\
  \times & \times \\
  \times & \times
\end{bmatrix}
}_{\text{C $\in \mathbb{R}^{m \times r}$ }}
\underbrace{
\begin{bmatrix}
  \times & \times & \times & \times & \times \\
  \times & \times & \times & \times & \times
\end{bmatrix}
}_{\text{X $\in \mathbb{R}^{r \times n}$ }}.
\end{equation}

\noindent There are also probabilistic versions of MF which incorporate probabilistic models to better handle uncertainty and variability in the data, leading to more accurate predictions. Such methodologies postulate a prior distribution over the latent factors and necessitate the computation of the posterior distribution to derive updated estimates. With that the matrix is not only decomposed but probabilistic interpretations of the factors are also possible. Despite considerable progression in the probabilistic versions, there is demand for such methods in applications such as uncertainty quantification, managing time-series data, and executing efficient probabilistic components. \newline

\noindent We should also note that some algorithms are suitable for sequential data - updating $C$ and $X$ incrementally as new data points are observed and thus incorporating temporal dynamics and sequential dependencies into the factorisation process, and others are non-sequential - treating the dataset as a batch independent of the time varying component. \newline

\noindent Throughout this thesis we are going to focus on probabilistic MF algorithms, along with their application to 12-lead ECG data, targeting the problem of managing high-dimensional time-series data. Some examples of such probabilistic MF algorithms are Probabilistic Matrix Factorisation (PMF) (\cite{NIPS2007_d7322ed7}), Dictionary filtering (\cite{cite-key}), Probabilistic Sequential Matrix Factorization (PSMF) (\cite{akyildiz2021probabilistic}), MLE-SMF?. \newline

\noindent The paper "Probabilistic matrix factorisation" (PMF) (\cite{NIPS2007_d7322ed7}) introduces an efficient and scalable probabilistic model for collaborative filtering. The algorithm performs well on large, sparse and imbalanced datasets. This is demonstrated by using a Netflix dataset, where PMF models the user preference matrix $R$ as a product of two lower-dimensional matrices: user feature matrix $U$ and movie feature matrix $V$. The conditional distribution over observed ratings is modeled using Gaussian noise, and zero-mean spherical Gaussian priors are placed on the user and movie feature vectors. The paper also presents two extensions to the initial PMF model: incorporating \textit{adaptive priors} to automatically control the model complexity through these priors over the model parameters, and a \textit{constrained PMF} version to handle and improve predictions for users with few ratings by incorporating constraints based on the assumption that users with similar movie ratings have similar preferences. The authors show that PMF significantly outperforms traditional Singular Value Decomposition (SVD) (\cite{4ba978eb-d878-342d-a11e-6d7554474b2d}) models and scales linearly with the number of observations. It's worth noting that PMF treats each rating as an independent event meaning the time varying component is not taken into consideration, making PMF a batch learning model designed to process large datasets in a non-sequential manner. \newline

\noindent In the paper "Dictionary Filtering: A Probabilistic Approach to Online Matrix Factorization" (\cite{cite-key}), the authors introduce a novel online MF algorithm known as dictionary filtering. It leverages probabilistic models, specifically using recursive linear filters, and efficiently factorises the original data matrix into a dictionary matrix and a coefficients matrix. This is an online and sequential algorithm, meaning it is suitable for high-dimensional and time-varying data, and it also has easy to tune parameters. Although the model can learn nonstationary and dynamic data, it is developed for linear and Gaussian state space models (SSM). 

(Particularly for ECG data, ECG has a nonlinear SSM which doesn't suit the dictionary filtering.) -> Efficient for high-dimensional data, adaptable to non-stationary environments, removes the need for step-size tuning; flexible and computationally efficient: $O(mr^2)$ independent of the number of data points. \newline

\noindent Later, Akyildiz et.al. develop Probabilistic Sequential Matrix Factorization (PSMF) (\cite{akyildiz2021probabilistic}). This method is tailored to time-varying and non-stationary datasets consisting of high-dimensional time-series. Nonlinear Gaussian SSM are considered, decomposing the original matrix into a dictionary matrix and time-variying coefficient matrix. This time, the matrices are with potentially nonlinear dependencies. The model is demonstrated to work on tasks such as forecasting, changepoint detection, missing data imputation, and is shown to work on real-world data with a periodic subspace. There is also a robust version using Student-t filters to handle model misspecification.

PSMF efficiently captures temporal dependencies through Markovian structures on the coefficients, making it possible to encode the dependencies into a lower dimensional latent space. Strengths: Handles non-stationary and time-varying data, robust to model misspecification, efficient sequen- tial inference. Suitable for: modelling data with periodic subpaces, high-dimensional data by reducing it to lower-dimensional latent space, data imputation,
Weaknesses: Potential problems with very large datasets, having many data points. \newline

% \noindent In the unpublished MSc thesis (Imperial College London) of Rina Maletta (\cite{rina}), the author introduces Matrix-Variate Gaussian Matrix Factorization (MVGMF). This is a novel PMF method using matrix-variate Gaussian distributions. The algorithm has fast Gaussian updates which take the form of a preconditioned MF algorithm which is stable. An extension for handling missing data and data imputation is also proposed. The method is tested on the Netflix Prize dataset, London air quality (NO2) data, and Olivetti face image dataset.

\section{Contributions}

We outline the contributions made in this thesis:

\begin{itemize}
    \item We show a novel application of PSMF to complex 12-lead ECG data
    \item We tackle three tasks: missing data imputation, R-peaks detection and forecast
    \item We show PSMF with Fourier basis with multiple Fourier terms and rank higher than 1
\end{itemize}

\noindent The main focus of the research project would be to explore PMF methods applied to high-dimensional time-series data with nonlinear subspace: both considering the time varying component, and treating the whole time series as a batch without considering the time component. More precisely, the project would focus on applying such methods to 12-lead ECG data which is 12-dimensional time-series data with a nonlinear subspace. Some of the tasks which could be explored include: modelling the data so that the future behaviour can be predicted, anomaly detection, imputing missing data, or performing peak detection. \newline

\noindent Using probabilistic methods (and specifically the PMF ones) on ECG data is not widely explored. Applying methods such as Probabilistic Matrix Factorisation (PMF) [1], Probabilistic Sequential Matrix Factorisation (PSMF) [2], Matrix-Variate Gaussian Matrix Factorization (MVGMF) [3], etc. to 12-lead ECG data would showcase novel real-world PMF applications in the medical field, and potentially improve the accuracy and efficiency of ECG data analysis. \newline

\section{Notation}

We are going to denote the original data matrix as $Y$, and let $Y \in \mathbb{R}^{m \times n}$. Let $C \in \mathbb{R}^{m \times r}$ be the dictionary matrix, $X \in \mathbb{R}^{r \times n}$ be the coefficients matrix, and $r$ be the approximation rank. \newline

\noindent With $I_d \in \mathbb{R}^{d \times d}$ we are going to denote the identity matrix, with $MN(X; M, U, V)$ the matrix normal distribution with $M$ the mean-matrix, $U$ the row-covariance, and $V$ the column covariance. Further, with $N(x; \mu, \Sigma)$  let's denote the Gaussian density with mean $\mu$ and $\Sigma$ the covariance matrix, $IG(s; \alpha, \beta)$ is the inverse gamma distribution with shape $\alpha$ and scale $\beta$.  \newline

\hline

\noindent  We denote the vectorised forms of the matrices with their respective lower case letters. We can formally define c = vec(C), where vec(·) is the vectorisation operation. The columns of C are stacked on each other, so if C ∈ Rm×r, then c ∈ Rmr×1. We can also define the inverse vectorisation operator C = vec−1(c). \newline

\noindent We have that p(x) denotes the probability density function (pdf) of x and p(y|x) is the conditional density of y given x. Where A ∈ Rm×n is a matrix, then MN(A;M,U, V ) denotes the matrix normal pdf for A, where M ∈ Rm×n is the mean, U ∈ Rn×n is the among-row co-variance and V ∈ Rm×m is the among-column co-variance. We also define the multivariate normal distribution in the following way; let a ∈ Rm, then N(a; μ, Σ) denotes the pdf of the multivariate normal distribution for the random variable a, where μ ∈ Rm is the mean and Σ ∈ Rm×m is the covariance matrix. Im ∈ Rm×m is the m×m identity matrix. \newline

\chapter{Background}

\section{Preliminaries}

\subsection{Matrix Normal Distribution} 

TODO 

Cite paper

\subsection{Kronecker Product}

\begin{definition}
Let $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{p \times q}$ be matrices. Then their \textit{Kronecker product} denoted as $A \otimes B \in \mathbb{R}^{mp \times nq}$ is given by (\cite{alma993596394401591})

\begin{equation}
    A \otimes B = \begin{bmatrix}
        a_{11}B & a_{12}B & \hdots & a_{1n}B \\
        a_{21}B & a_{22}B & \hdots & a_{2n}B \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1}B & a_{m2}B & \hdots & a_{mn}B
    \end{bmatrix},
\end{equation}
where $\{a_{ij}\}$, $i \in \{1,...,m\}$, $j \in \{1, ..., n\}$ are the elements of $A$.
\end{definition}

\subsection{Fourier Series}

TODO

\section{PSMF}

Intro again?

\subsection{PSMF Model}

For observations $(y_k)_{k \geq 1} \in \mathbb{R}^{m}$, latent coefficients $(x_k)_{k \geq 1} \in \mathbb{R}^{r}$, and a dictionart matrix $C \in \mathbb{R}^{m \times r}$ the PSMF model can be described with the following state-space equations:

\begin{equation}
    p(C) = \mathcal{MN}(C; C_0, I_d, V_0)
\end{equation}
\begin{equation}
    p(x_0) = \mathcal{N}(x_0; \mu_0, P_0)
\end{equation}
\begin{equation}
    p(x_k \mid x_{k-1}) = \mathcal{N}(x_k; f_{\theta}(x_{k-1}), Q_k)
\end{equation}
\begin{equation}
    p(y_k \mid x_k, C) = \mathcal{N}(y_k; Cx_k, R_k)
\end{equation}

\hline

- **Dictionary Prior**:
  \[
  p(C) = \mathcal{MN}(C; C_0, I_d, V_0)
  \]
  where \( \mathcal{MN} \) denotes the matrix-variate Gaussian distribution, \( C_0 \) is the mean matrix, \( I_d \) is the identity matrix of appropriate dimension, and \( V_0 \) is the covariance matrix.

- **Initial State of Coefficients**:
  \[
  p(x_0) = \mathcal{N}(x_0; \mu_0, P_0)
  \]
  where \( \mu_0 \) and \( P_0 \) are the mean and covariance of the initial state.

- **State Transition**:
  \[
  p(x_k \mid x_{k-1}) = \mathcal{N}(x_k; f_{\theta}(x_{k-1}), Q_k)
  \]
  \( f_{\theta} \) is a nonlinear function parameterized by \( \theta \), and \( Q_k \) is the process noise covariance.

- **Observation Model**:
  \[
  p(y_k \mid x_k, C) = \mathcal{N}(y_k; Cx_k, R_k)
  \]
  where \( R_k \) is the observation noise covariance.

The likelihood function associated with observing all the data \( Y \) given the latent variables and parameters is computed as:
\[
p(Y \mid X, C, \theta) = \prod_{k=1}^n \mathcal{N}(y_k; Cx_k, R_k)
\]

\subsection{PSMF Inference}

The inference process in PSMF involves the following steps, each corresponding to the elements of the probabilistic model.

- **Prediction**:
  \[
  p(x_k \mid y_{1:k-1}) = \int p(x_k \mid x_{k-1}) p(x_{k-1} \mid y_{1:k-1}) dx_{k-1}
  \]

- **Update**: After computing the predictive distribution of \(x_k\), the update steps for \(x_k\) and \(C\) are given by:
  \[
  p(y_k \mid y_{1:k-1}) = \int \int p(y_k \mid C, x_k) p(x_k \mid y_{1:k-1}) p(C \mid y_{1:k-1}) dx_k dC
  \]
  \[
  p(x_k \mid y_{1:k}) = \frac{p(x_k \mid y_{1:k-1}) p(y_k \mid x_k, y_{1:k-1})}{p(y_k \mid y_{1:k-1})}
  \]
  \[
  p(C \mid y_{1:k}) = \frac{p(C \mid y_{1:k-1}) p(y_k \mid C, y_{1:k-1})}{p(y_k \mid y_{1:k-1})}
  \]

\subsection{PSMF Algorithm}

\begin{algorithm}
\caption{Probabilistic Sequential Matrix Factorization (PSMF)}
\begin{algorithmic}[1]
\State Initialize \( C_0, V_0, \mu_0, P_0, (Q_k)_{k \geq 1}, (R_k)_{k \geq 1} \)
\For{\( k = 1 \) to \( n \)}
    \State \textbf{Prediction:}
    \State \( p(x_k \mid y_{1:k-1}) = \int p(x_k \mid x_{k-1}) p(x_{k-1} \mid y_{1:k-1}) dx_{k-1} \)
    \State \textbf{Update:}
    \State \( p(y_k \mid y_{1:k-1}) = \int \int p(y_k \mid C, x_k) p(x_k \mid y_{1:k-1}) p(C \mid y_{1:k-1}) dx_k dC \)
    \State \( p(x_k \mid y_{1:k}) = \frac{p(x_k \mid y_{1:k-1}) p(y_k \mid x_k, y_{1:k-1})}{p(y_k \mid y_{1:k-1})} \)
    \State \( p(C \mid y_{1:k}) = \frac{p(C \mid y_{1:k-1}) p(y_k \mid C, y_{1:k-1})}{p(y_k \mid y_{1:k-1})} \)
\EndFor
\end{algorithmic}
\end{algorithm}


\section{rPSMF}

\subsection{rPSMF Model}

The robust variant, rPSMF, incorporates robustness to outliers and model misspecifications by integrating a heavy-tailed t-distribution into the framework.

- **Scale Mixing Prior**:
  \[
  p(s) = \text{IG}(s; \lambda_0 / 2, \lambda_0 / 2)
  \]
  where \( \text{IG} \) denotes the inverse gamma distribution and \( \lambda_0 \) is a scale parameter.

- **Modified Dictionary and State Transitions**:
  \[
  p(C \mid s) = \mathcal{MN}(C; C_0, I_d, sV_0)
  \]
  \[
  p(x_0 \mid s) = \mathcal{N}(x_0; \mu_0, sP_0)
  \]
  \[
  p(x_k \mid x_{k-1}, s) = \mathcal{N}(x_k; f_{\theta}(x_{k-1}), sQ_k)
  \]
  \[
  p(y_k \mid x_k, C, s) = \mathcal{N}(y_k; Cx_k, sR_k)
  \]

The likelihood for rPSMF considering the robust modifications and the scale variable \( s \) is:
\[
p(Y \mid X, C, s, \theta) = \prod_{k=1}^n \mathcal{N}(y_k; Cx_k, sR_k)
\]


\begin{align*}
p(s) &= \mathcal{IG}(s; \lambda_0/2, \lambda_0/2) \\
p(\mathbf{C} | s) &= \mathcal{MN}(\mathbf{C}; \mathbf{C}_0, \mathbf{I}_d, s\mathbf{V}_0) \\
p(\mathbf{x}_0 | s) &= \mathcal{N}(\mathbf{x}_0; \boldsymbol{\mu}0, s\mathbf{P}0) \\
p{\theta}(\mathbf{x}k | \mathbf{x}{k-1}, s) &= \mathcal{N}(\mathbf{x}k; f{\theta}(\mathbf{x}{k-1}), s\mathbf{Q}_0) \\
p(\mathbf{y}_k | \mathbf{x}_k, \mathbf{C}, s) &= \mathcal{N}(\mathbf{y}_k; \mathbf{C}\mathbf{x}_k, s\mathbf{R}_0)
\end{align*}

\subsection{rPSMF Inference}

Inference in rPSMF incorporates the handling of the robust parameters and scale mixing variable \( s \) introduced to manage outliers and model misspecifications.

- **Prediction**:
  \[
  p(x_k \mid y_{1:k-1}, s) = \int p(x_k \mid x_{k-1}, s) p(x_{k-1} \mid y_{1:k-1}, s) dx_{k-1}
  \]

- **Update**: This part involves the update of \(x_k\), \(C\), and \(s\) using robust estimation methods:
  \[
  p(y_k \mid y_{1:k-1}, s) = \int \int p(y_k \mid C, x_k, s) p(x_k \mid y_{1:k-1}, s) p(C \mid y_{1:k-1}, s) dx_k dC
  \]
  \[
  p(x_k \mid y_{1:k}, s) = \frac{p(x_k \mid y_{1:k-1}, s) p(y_k \mid x_k, y_{1:k-1}, s)}{p(y_k \mid y_{1:k-1}, s)}
  \]
  \[
  p(C \mid y_{1:k}, s) = \frac{p(C \mid y_{1:k-1}, s) p(y_k \mid C, y_{1:k-1}, s)}{p(y_k \mid y_{1:k-1}, s)}
  \]
  \[
  p(s \mid y_{1:k}) = \text{IG}(s; \text{updated parameters based on data likelihood})
  \]

\subsection{rPSMF Algorithm}

\begin{algorithm}
\caption{Robust Probabilistic Sequential Matrix Factorization (rPSMF)}
\begin{algorithmic}[1]
\State Initialize \( C_0, V_0, \mu_0, P_0, (Q_k)_{k \geq 1}, (R_k)_{k \geq 1}, s_0, \lambda_0 \)
\For{\( k = 1 \) to \( n \)}
    \State \textbf{Prediction:}
    \State \( p(x_k \mid y_{1:k-1}, s) = \int p(x_k \mid x_{k-1}, s) p(x_{k-1} \mid y_{1:k-1}, s) dx_{k-1} \)
    \State \textbf{Update:}
    \State \( p(y_k \mid y_{1:k-1}, s) = \int \int p(y_k \mid C, x_k, s) p(x_k \mid y_{1:k-1}, s) p(C \mid y_{1:k-1}, s) dx_k dC \)
    \State \( p(x_k \mid y_{1:k}, s) = \frac{p(x_k \mid y_{1:k-1}, s) p(y_k \mid x_k, y_{1:k-1}, s)}{p(y_k \mid y_{1:k-1}, s)} \)
    \State \( p(C \mid y_{1:k}, s) = \frac{p(C \mid y_{1:k-1}, s) p(y_k \mid C, y_{1:k-1}, s)}{p(y_k \mid y_{1:k-1}, s)} \)
    \State Update \( s \) using the robust estimation method
\EndFor
\end{algorithmic}
\end{algorithm}

\section{PSMF for Handling Missing Data}

Given:
\begin{itemize}
    \item A data matrix \( Y \in \mathbb{R}^{d \times n} \) with missing entries
    \item A dictionary matrix \( C \in \mathbb{R}^{d \times r} \)
    \item Latent coefficients matrix \( X \in \mathbb{R}^{r \times n} \)
\end{itemize}

\subsection*{Model Definition}
\begin{align*}
    p(C) & = \mathcal{MN}(C; C_0, I_d, V_0) \quad \text{(Matrix-variate Gaussian prior on dictionary)} \\
    p(x_0) & = \mathcal{N}(x_0; \mu_0, P_0) \quad \text{(Gaussian prior on initial coefficients)} \\
    p(x_k | x_{k-1}) & = \mathcal{N}(x_k; f_{\theta}(x_{k-1}), Q_k) \quad \text{(State transition model)} \\
    p(y_k | x_k, C) & = \mathcal{N}(y_k; Cx_k, R_k) \quad \text{(Observation model)} \\
    p(s) & = \text{IG}(s; \lambda_0 / 2, \lambda_0 / 2) \quad \text{(Scale mixing variable for rPSMF)}
\end{align*}

\subsection*{Missing Data Imputation}
Missing entries in \( Y \) are handled by:
\begin{itemize}
    \item Assuming \( Y_{\text{obs}} \) are the observed entries
    \item \( Y_{\text{miss}} \) are the missing entries
\end{itemize}

The likelihood for the observed data is given by:
\[
p(Y_{\text{obs}} | X, C) = \prod_{k=1}^n \mathcal{N}(y_{k,\text{obs}}; Cx_k, R_{k,\text{obs}})
\]

where \( R_{k,\text{obs}} \) is the covariance matrix adjusted for the observed dimensions of \( y_k \).

\subsection*{Objective}
Maximize the likelihood of the observed data while appropriately estimating the missing data through the model's latent structure:
\[
\max_{X, C} \log p(Y_{\text{obs}} | X, C)
\]

\subsection{Model}

\subsection{Inference}

\subsection{Algorithm}

\begin{algorithm}
\caption{Missing Data Imputation using PSMF and rPSMF}
\begin{algorithmic}[1]
\State Initialize \( C_0, V_0, \mu_0, P_0, s_0 \) (for rPSMF), \( (Q_k)_{k \geq 1}, (R_k)_{k \geq 1} \)
\State Set model parameters based on whether using PSMF or rPSMF
\For{\( k = 1 \) to \( n \)}
    \State \textbf{Predict Next State:}
    \State \( p(x_k \mid y_{1:k-1}) = \int p(x_k \mid x_{k-1}) p(x_{k-1} \mid y_{1:k-1}) dx_{k-1} \)
    \State \textbf{Update Estimates:}
    \State Incorporate observations \( y_k \), handling missing data via the model's structure
    \State \( p(y_k \mid y_{1:k-1}) = \int \int p(y_k \mid C, x_k) p(x_k \mid y_{1:k-1}) p(C \mid y_{1:k-1}) dx_k dC \)
    \State \( p(x_k \mid y_{1:k}) = \frac{p(x_k \mid y_{1:k-1}) p(y_k \mid x_k, y_{1:k-1})}{p(y_k \mid y_{1:k-1})} \)
    \State \( p(C \mid y_{1:k}) = \frac{p(C \mid y_{1:k-1}) p(y_k \mid C, y_{1:k-1})}{p(y_k \mid y_{1:k-1})} \)
    \State Update \( s \) if using rPSMF
    \State \textbf{Impute Missing Data:}
    \State Utilize the updated estimates of \( x_k \) and \( C \) to impute missing entries in \( y_k \)
\EndFor
\State \textbf{Output:} Imputed matrix \( Y \) with missing entries filled
\end{algorithmic}
\end{algorithm}


\section{ECG/medical introduction}

The ECG is a crucial diagnostic tool in clinical practice. It is especially useful in diagnosing rhythm disturbances, changes in electrical conduction, and myocardial ischemia and infarction.

As cardiac cells depolarize and repolarize, electrical currents spread throughout the body because the tissues surrounding the heart are able to conduct electrical currents generated by the heart. When these electrical currents are measured by an array of electrodes placed at specifi c locations on the body surface, the recorded tracing is called an ECG (Fig. 2.13). The repeating waves of the ECG represent the sequence of depolarization and repolarization of the atria and ventricles. The ECG does not measure absolute voltages, but voltage changes from a baseline (isoelectric) voltage. ECGs are generally recorded on paper at a speed of 25 mm/s and with a vertical calibration of 1 mV/cm.

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/ecg.png}
\caption{Components of the ECG trace. An enlargement of one of the repeating waveform units in the rhythm strip shows the P wave, QRS complex, and T wave, which represent atrial depolarization, ventricular depolarization, and ventricular repolarization, respectively. The PR interval represents the time required for the depolarization wave to transverse the atria and the AV node; the QT interval represents the period of ventricular depolarization and repolarization; and the ST segment is the isoelectric period when the entire ventricle is depolarized. Each small square is 1 mm.}
\label{fig:r-peaks}
\end{figure}

\hline

A 12-Lead ECG (Electrocardiogram) is a standard diagnostic tool used to assess the electrical activity of the heart. It provides a comprehensive view of the heart's electrical activity from different angles. The 12 leads consist of:

\begin{itemize}
  \item 6 precordial leads (V1-V6) placed on the chest
  \item 3 limb leads (I, II, III)
  \item 3 augmented limb leads (aVR, aVL, aVF)
\end{itemize}

Each lead records the heart's electrical activity from a different perspective, providing information on:

\begin{itemize}
  \item Heart rate and rhythm
  \item Conduction abnormalities
  \item Chamber enlargement
  \item Myocardial ischemia or infarction
  \item Electrolyte imbalances
\end{itemize}

The ECG machine typically records data for about 10 seconds, producing a graph of voltage versus time.

The Electrocardiogram (ECG) is a crucial diagnostic tool in cardiology that records the electrical activity of the heart. A standard 12-lead ECG system provides multiple perspectives of cardiac electrical activity, facilitating comprehensive analysis.

Signal Acquisition: ECG data is acquired through electrodes placed on the body surface, measuring potential differences over time.
Waveform Components: The ECG waveform consists of P, Q, R, S, and T waves, each representing specific cardiac events.
Lead System: The 12-lead system comprises:

6 precordial leads (V1-V6)
3 limb leads (I, II, III)
3 augmented limb leads (aVR, aVL, aVF)

add images sources

\begin{figure}[h!]
    \centering
    \subfigure{
        \includegraphics[width=0.48\linewidth]{images/ECG_Electrode_Placement.png}
        \label{fig:ecg-electrode}
    }
    \hfill
    \subfigure{
        \includegraphics[width=0.48\linewidth]{images/SinusRhythmLabels.png}
        \label{fig:sinus-rhythm}
    }
    \caption{Left: . Right: }
    \label{fig:combined}
\end{figure}

PQRST complex, R-peak..., diagram, data 

% forcing a page break
\clearpage

\chapter{Experiemnts and Results}

Add citations 

The research aims to use the already available A Large Scale 12-lead Electrocardiogram Database for Arrhythmia Study [5], [6], [7]. This is a comprehensive database of high-quality 12-lead ECG signals collected from 45,152 patients. Each signal is with length of 10 seconds corresponding to 5000 data points. The dataset is designed to support arrhythmia research, containing labeled data for various cardiac conditions such as atrial fibrillation, premature ventricular contractions, and bundle branch blocks. This large dataset has high-quality labels from professional experts, diverse arrhythmia types, and additional cardiovascular conditions, making it suitable for performing different research tasks on it without spending time on gathering, cleaning and processing data. \newline


• Presents a comprehensive database of 12-lead ECG signals collected from 45,152 patients. Each signal is with length of 10 seconds corresponding to 5000 data points.
• The dataset is designed to support arrhythmia research, containing labeled data for various cardiac conditions such as atrial fibrillation, premature ventricular contractions, and bundle branch blocks.
• ECG data is non-invasive and critical for diagnosing cardiovascular conditions, but analyzing such large
datasets requires efficient automated methods.
• Large dataset, high-quality labels from professional experts, diverse arrhythmia types, and additional cardiovascular conditions. \newline


Pre,post, processing

What exactly is tested - 5k points, sample from the points, results, smoothing, no smoothing, differеnt ranks, fourier terms

\section{Missing data}

Both algorithms -> psmf, rpsmf, mle-smf, mvgmf?

results and images \newline

%% DO NOT EDIT - AUTOMATICALLY GENERATED FROM RESULTS!
%% This table requires booktabs and multirow!
%% Table for missing percentage 20
\begin{table}[h!]
    \centering
    \begin{threeparttable}
        \begin{tabular}{lccc}
            \hline
             & 20\% & 30\% & 40\% \\
            \hline
            PSMF & 0.39 & 0.32 & 0.24 \\
            rPSMF & \textbf{0.85} & \textbf{0.78} & \textbf{0.71} \\
            MLE-SMF & 0.18 & 0.16 & 0.15 \\
            \hline
        \end{tabular}
        \caption{Average coverage proportion of the missing data by the $2\sigma$ uncertainty bars of the posterior predictive estimates, averaged over 100 repetitions.}
    \end{threeparttable}
\end{table}

\begin{table}[h!]
\centering
\label{tab:my_label}
\begin{tabular}{@{}lccc|ccccc@{}}
\toprule
 & \multicolumn{3}{c}{Imputation RMSE} & \multicolumn{3}{c}{Runtime (s)} \\
 & 20\% & 30\% & 40\% & 20\% & 30\% & 40\% \\
\midrule
PSMF & $\underset{{\scriptscriptstyle \;\;(20.80)}}{63.93}$ & $\underset{{\scriptscriptstyle \;\;(22.15)}}{80.90}$ & $\underset{{\scriptscriptstyle \;\;\;(21.30)}}{104.68}$ & 1.05 & 1.25 & 1.19 \\
rPSMF & $\underset{{\scriptscriptstyle \;\;(17.29)}}{\textbf{60.58}}$ & $\underset{{\scriptscriptstyle \;\;(18.50)}}{\textbf{74.56}}$ & $\underset{{\scriptscriptstyle \;\;(20.12)}}{\textbf{93.22}}$ & 1.22 & 1.31 & 1.17 \\
MLE-SMF & $\underset{{\scriptscriptstyle \;\;\;(22.15)}}{255.87}$ & $\underset{{\scriptscriptstyle \;\;\;(67.00)}}{263.76}$ & $\underset{{\scriptscriptstyle \;\;\;(81.31)}}{271.45}$ & 6.17 & 1.00 & 1.12 \\
TMF & $\underset{{\scriptscriptstyle \;\;\;(15.78)}}{169.83}$ & $\underset{{\scriptscriptstyle \;\;\;(16.33)}}{167.91}$ & $\underset{{\scriptscriptstyle \;\;\;(17.91)}}{155.58}$ & 2.11 & 0.50 & 0.58 \\
\bottomrule
\end{tabular}
\caption{Imputation error and runtime using 20\%, 30\% and 40\% missing values, averaged over 100 random repetitions.}
\end{table}

TODO: add figures showing the original data, missing parts and imputed parts

\section{R peaks?}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/r_peaks.png}
\label{fig:r-peaks}
\end{figure}

When forecasting... its hard to model the sharp peaks... smoother -> remove reconstrution and set threshold and get the r-peaks

Original - Forecast -> for R peaks detection

One paper about NMF

Scipy function

remove the reconstruction from the original signal

\section{Forecasting}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=1]{images/periodic_fit_1500_6_6_150it.pdf}
\caption{My caption here}
\label{somelabelforreference}
\end{center}
\end{figure}

This fails 

- issues, R-peak

Lower frequency is better -> rank 6, fourier terms 6, iterations 100, 200, 500, leraning rate


\chapter{Conclusion}

data imputation 

forecasting challneged or success

R-peaks

\section{Future work}

More complex model etc...

Faster computation, etc...

Conclusion goes here. 





\clearpage
 %% reset page counter and start appendix pages with A
\pagenumbering{arabic}
\renewcommand*{\thepage}{A\arabic{page}}

%% Appendix goes here
%\appendix
%
%\chapter{Appendix title}
%
%Appendix goes here.


%%References part of appendices
% References: modify the file refs.bib
\bibliographystyle{plainnat}
\bibliography{refs}


\end{document}
